{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 備份&把資料庫資料讀進來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (3.11.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb://admin:widmwidm9527@140.115.54.44:27017')\n",
    "db = client['ETL-api-creator']\n",
    "time_result = time.strftime(\"%Y-%m-%d %Hh%Mm%Ss\", time.localtime())\n",
    "\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb://admin:widmwidm9527@140.115.54.44:27017')\n",
    "\n",
    "collection1 = db['KKBOX_Label_Ken1']\n",
    "cursor1 = collection1.find({}) \n",
    "data1 = [d for d in cursor1] \n",
    "\n",
    "collection2 = db['KKBOX_Label_Ken2']\n",
    "cursor2 = collection2.find({}) \n",
    "data2 = [d for d in cursor2] \n",
    "# #make directory\n",
    "# os.mkdir(time_result)\n",
    "# os.chdir(time_result)\n",
    "# #write json\n",
    "# print('紀錄時間', time.strftime(\"%Y/%m/%d-%H:%M:%S\", time.localtime()))\n",
    "    \n",
    "# with open('label1.json', 'w', encoding='utf8') as file:\n",
    "#     count = 0\n",
    "#     for d in data1:\n",
    "#         d.pop('_id')\n",
    "#         ret = json.dumps(d, ensure_ascii=False)\n",
    "#         file.write(ret)\n",
    "#         file.write('\\n')\n",
    "#         count+=1\n",
    "#     print('data1 共寫了{}個文'.format(count))\n",
    "    \n",
    "# with open('label2.json', 'w', encoding='utf8') as file:\n",
    "#     count = 0\n",
    "#     for d in data2:\n",
    "#         d.pop('_id')\n",
    "#         ret = json.dumps(d, ensure_ascii=False)\n",
    "#         file.write(ret)\n",
    "#         file.write('\\n')\n",
    "#         count+=1\n",
    "#     print('data2 共寫了{}個文'.format(count))\n",
    "# os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "姵芸標了998筆\n",
      "怡萱標了1000筆\n",
      "慕耘標了999筆\n",
      "意慈標了1000筆\n",
      "皆有標記的資料數量: 1998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data1 = sorted(data1, key = lambda x: int(x['sentence_id']))\n",
    "data2 = sorted(data2, key = lambda x: int(x['sentence_id']))\n",
    "data1time = []\n",
    "data1seq = []\n",
    "data1asplen, data2asplen = [], []\n",
    "for i in data1:\n",
    "    data1time.append(i['time'])\n",
    "    data1asplen.append(len(i['aspect']))\n",
    "data2time = []\n",
    "data2seq = []\n",
    "for i in data2:\n",
    "    data2time.append(i['time'])\n",
    "    data2asplen.append(len(i['aspect']))\n",
    "data1asplen = np.array(data1asplen)\n",
    "data2asplen = np.array(data2asplen)\n",
    "print('姵芸標了{}筆'.format(sum(data1asplen[:1000]>0)))\n",
    "print('怡萱標了{}筆'.format(sum(data1asplen[1000:]>0)))\n",
    "print('慕耘標了{}筆'.format(sum(data2asplen[:1000]>0)))\n",
    "print('意慈標了{}筆'.format(sum(data2asplen[1000:]>0)))\n",
    "\n",
    "labeldata = []\n",
    "for i in range(2000):\n",
    "    if len(data1[i]['aspect'])>0 and len(data2[i]['aspect'])>0:\n",
    "        labeldata.append(i)\n",
    "print('皆有標記的資料數量:', len(labeldata))#因為極少數的資料真的沒有可以標記的地方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 統計各項Entity資訊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Entity Base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. product name length: 2.771963430561602\n",
      "avg. product brand length: 3.2907431551499347\n",
      "avg. aspect term length: 3.2907431551499347\n",
      "avg. opinion word length: 3.2907431551499347\n",
      "avg. product name number: 24.0\n",
      "avg. product brand number: 20.0\n",
      "avg. aspect term number: 23.0\n",
      "avg. opinion word number: 27.0\n"
     ]
    }
   ],
   "source": [
    "#統計各Entity平均長度\n",
    "pn_len = []\n",
    "pb_len = []\n",
    "at_len = []\n",
    "ow_len = []\n",
    "for dta in data1:\n",
    "    for asp in dta['aspect']:\n",
    "        if asp['product_name']!='':\n",
    "            pn_len.append(len(asp['product_name']))\n",
    "        if asp['product_brand']!='':\n",
    "            pb_len.append(len(asp['product_brand']))\n",
    "        if asp['aspect_term']!='':\n",
    "            at_len.append(len(asp['aspect_term']))\n",
    "        if asp['opinion_word']!='':\n",
    "            ow_len.append(len(asp['opinion_word']))\n",
    "for dta in data2:\n",
    "    for asp in dta['aspect']:\n",
    "        if asp['product_name']!='':\n",
    "            pn_len.append(len(asp['product_name']))\n",
    "        if asp['product_brand']!='':\n",
    "            pb_len.append(len(asp['product_brand']))\n",
    "        if asp['aspect_term']!='':\n",
    "            at_len.append(len(asp['aspect_term']))\n",
    "        if asp['opinion_word']!='':\n",
    "            ow_len.append(len(asp['opinion_word']))\n",
    "print('avg. product name length:', np.mean(pn_len))\n",
    "print('avg. product brand length:', np.mean(pb_len))\n",
    "print('avg. aspect term length:', np.mean(pb_len))\n",
    "print('avg. opinion word length:', np.mean(pb_len))\n",
    "avg_pn = []\n",
    "avg_pb = []\n",
    "avg_at = []\n",
    "avg_ow = []\n",
    "for i in range(len(data1)):\n",
    "    avg_pn.append(int(len(dta['sentence'])/np.mean(pn_len)))\n",
    "    avg_pb.append(int(len(dta['sentence'])/np.mean(pb_len)))\n",
    "    avg_at.append(int(len(dta['sentence'])/np.mean(at_len)))\n",
    "    avg_ow.append(int(len(dta['sentence'])/np.mean(ow_len)))\n",
    "print('avg. product name number:', np.mean(avg_pn)) #一句話裡面有幾個PN長度的entity\n",
    "print('avg. product brand number:', np.mean(avg_pb)) #一句話裡面有幾個PB長度的entity\n",
    "print('avg. aspect term number:', np.mean(avg_at)) #一句話裡面有幾個AT長度的entity\n",
    "print('avg. opinion word number:', np.mean(avg_ow)) #一句話裡面有幾個OW長度的entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_name\n",
      "Origin cohen kappa: -0.04846367114442529\n",
      "Origin f1 score: 0.9507969507969508\n",
      "Add Negative Information cohen kappa: 0.9480189487879064\n",
      "Add Negative Information f1 score: 0.9507969507969508\n",
      "Data1 label entity:2903; Data2 label entity:2869\n",
      "Entity Intersection:2744; Union:3028\n",
      "\n",
      "product_brand\n",
      "Origin cohen kappa: -0.04846367114442529\n",
      "Origin f1 score: 0.9507969507969508\n",
      "Add Negative Information cohen kappa: 0.9480189487879064\n",
      "Add Negative Information f1 score: 0.9507969507969508\n",
      "Data1 label entity:6273; Data2 label entity:6252\n",
      "Entity Intersection:5488; Union:6056\n",
      "\n",
      "aspect_term\n",
      "Origin cohen kappa: -0.04846367114442529\n",
      "Origin f1 score: 0.9507969507969508\n",
      "Add Negative Information cohen kappa: 0.9480189487879064\n",
      "Add Negative Information f1 score: 0.9507969507969508\n",
      "Data1 label entity:10205; Data2 label entity:11125\n",
      "Entity Intersection:8232; Union:9084\n",
      "\n",
      "opinion_word\n",
      "Origin cohen kappa: -0.04846367114442529\n",
      "Origin f1 score: 0.9507969507969508\n",
      "Add Negative Information cohen kappa: 0.9480189487879064\n",
      "Add Negative Information f1 score: 0.9507969507969508\n",
      "Data1 label entity:12488; Data2 label entity:14339\n",
      "Entity Intersection:10976; Union:12112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#計算各Entity標記一致性(entity base)\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import f1_score\n",
    "data1_pn = []\n",
    "data2_pn = []\n",
    "data1_num, data2_num = 0, 0\n",
    "intersection, union = 0, 0\n",
    "\n",
    "TARGETS = ['product_name', 'product_brand', 'aspect_term', 'opinion_word']\n",
    "for TARGET in TARGETS:\n",
    "    print(TARGET)\n",
    "    for dta in data1:\n",
    "        temp_pn = []\n",
    "        for asp in dta['aspect']:\n",
    "            if asp[TARGET]!='':\n",
    "                temp_pn.append(asp[TARGET]+str(asp[TARGET+'_start']))\n",
    "        data1_num+=len(set(temp_pn))\n",
    "        data1_pn.append(set(temp_pn))\n",
    "    for dta in data2:\n",
    "        temp_pn = []\n",
    "        for asp in dta['aspect']:\n",
    "            if asp[TARGET]!='':\n",
    "                temp_pn.append(asp[TARGET]+str(asp[TARGET+'_start']))\n",
    "        data2_num+=len(set(temp_pn))\n",
    "        data2_pn.append(set(temp_pn))\n",
    "    pn1_kappa, pn2_kappa = [], []\n",
    "\n",
    "    for i in range(2000):\n",
    "        or_ = list(data1_pn[i].union(data2_pn[i]))\n",
    "        and_ = list(data1_pn[i].intersection(data2_pn[i]))\n",
    "        union+=len(or_)\n",
    "        intersection+=len(and_)\n",
    "        for temp in or_:\n",
    "            if temp in data1_pn[i]:\n",
    "                pn1_kappa.append(True)\n",
    "            else:\n",
    "                pn1_kappa.append(False)\n",
    "            if temp in data2_pn[i]:\n",
    "                pn2_kappa.append(True)\n",
    "            else:\n",
    "                pn2_kappa.append(False)\n",
    "    print('Origin cohen kappa:',cohen_kappa_score(pn1_kappa, pn2_kappa))\n",
    "    print('Origin f1 score:',f1_score(pn1_kappa, pn2_kappa))\n",
    "    #add more inofrmation\n",
    "    for i in range(sum(avg_ow)-len(pn1_kappa)):\n",
    "        pn1_kappa.append(False)\n",
    "        pn2_kappa.append(False)\n",
    "    print('Add Negative Information cohen kappa:',cohen_kappa_score(pn1_kappa, pn2_kappa))\n",
    "    print('Add Negative Information f1 score:',f1_score(pn1_kappa, pn2_kappa))\n",
    "    print('Data1 label entity:{}; Data2 label entity:{}'.format(data1_num, data2_num))\n",
    "    print('Entity Intersection:{}; Union:{}'.format(intersection, union))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Base (count kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先對每句都轉換成標記結果(不考慮不同aspect，全部壓到同句)\n",
    "def transfer_to_seq(data, pn=True, pb=True, at=True, ow=True, delBIOES=False, acsent=False):\n",
    "    ac_matrix = []\n",
    "    categories ={'功能':0, '品質':1,'配件':2,'售後':3,'外觀':4,'價位':5,'音量':6}\n",
    "    sentiment = ['none']*len(data['sentence'])\n",
    "    category = ['none']*len(data['sentence'])\n",
    "    labelseq = ['O']*len(data['sentence'])\n",
    "    for i in range(len(data['sentence'])):\n",
    "        ac_matrix.append(7*[0])\n",
    "    #for all labeled aspect\n",
    "    if pn:\n",
    "        for asp in data['aspect']:\n",
    "            try:\n",
    "                if asp['product_name_start']>=0:\n",
    "                    flag = True\n",
    "                    for idx in range(len(asp['product_name'])):\n",
    "                        if labelseq[asp['product_name_start']+idx]!='O':\n",
    "                            flag = False\n",
    "                    if flag:\n",
    "                        labelseq[asp['product_name_start']]='B-PN'\n",
    "                        for idx in range(len(asp['product_name'])-1):\n",
    "                            labelseq[asp['product_name_start']+idx+1]='I-PN'\n",
    "                        labelseq[asp['product_name_start']+len(asp['product_name'])-1]='E-PN'\n",
    "                        if len(asp['product_name'])==1:\n",
    "                            labelseq[asp['product_name_start']]='S-PN'\n",
    "            except:\n",
    "                print(data['sentence_id'], 'pn', '{}+{}>{}'.format(asp['product_name_start'], len(asp['product_name']), len(labelseq)))\n",
    "                \n",
    "    if pb:\n",
    "        for asp in data['aspect']:\n",
    "            try:\n",
    "                if asp['product_brand_start']>=0:\n",
    "                    flag = True\n",
    "                    for idx in range(len(asp['product_brand'])):\n",
    "                        if labelseq[asp['product_brand_start']+idx]!='O':\n",
    "                            flag = False\n",
    "                    if flag:\n",
    "                        labelseq[asp['product_brand_start']]='B-PB'                        \n",
    "                        for idx in range(len(asp['product_brand'])-1):\n",
    "                            labelseq[asp['product_brand_start']+idx+1]='I-PB'\n",
    "                        labelseq[asp['product_brand_start']+len(asp['product_brand'])-1]='E-PB'\n",
    "                        if len(asp['product_brand'])==1:\n",
    "                            labelseq[asp['product_brand_start']]='S-PB'\n",
    "                    if acsent: \n",
    "                        #for aspect category\n",
    "                        for idx in range(len(asp['product_brand'])):\n",
    "                            sentiment[asp['product_brand_start']+idx]=asp['aspect_category']\n",
    "                        #for sentiment\n",
    "                        for idx in range(len(asp['product_brand'])):\n",
    "                            category[asp['product_brand_start']+idx]=asp['sentiment']\n",
    "                            \n",
    "            except:\n",
    "                print(data['sentence_id'], 'pb', '{}+{}>{}'.format(asp['product_brand_start'], len(asp['product_brand']), len(labelseq), asp['product_brand']))\n",
    "    if at:\n",
    "        for asp in data['aspect']:\n",
    "            try:\n",
    "                if asp['aspect_term_start']>=0:\n",
    "                    #check first\n",
    "                    flag = True\n",
    "                    for idx in range(len(asp['aspect_term'])):\n",
    "                        if labelseq[asp['aspect_term_start']+idx]!='O':\n",
    "                            flag = False\n",
    "                    if flag:\n",
    "                        labelseq[asp['aspect_term_start']]='B-AT'\n",
    "                        for idx in range(len(asp['aspect_term'])-1):\n",
    "                            labelseq[asp['aspect_term_start']+idx+1]='I-AT'\n",
    "                        labelseq[asp['aspect_term_start']+len(asp['aspect_term'])-1]='E-AT'\n",
    "                        if len(asp['aspect_term'])==1:\n",
    "                            labelseq[asp['aspect_term_start']]='S-AT'\n",
    "            except:\n",
    "                print(data['sentence_id'], 'at', '{}+{}>{} : '.format(asp['aspect_term_start'], len(asp['aspect_term']), len(labelseq), asp['aspect_term']))\n",
    "\n",
    "    if ow:\n",
    "        for asp in data['aspect']:\n",
    "            try:\n",
    "                if asp['opinion_word_start']>=0:\n",
    "                    #check first\n",
    "                    flag = True\n",
    "                    for idx in range(len(asp['opinion_word'])):\n",
    "                        if labelseq[asp['opinion_word_start']+idx]!='O':\n",
    "                            flag = False\n",
    "                    if flag:\n",
    "                        labelseq[asp['opinion_word_start']]='B-OW'\n",
    "                        for idx in range(len(asp['opinion_word'])-1):\n",
    "                            labelseq[asp['opinion_word_start']+idx+1]='I-OW'\n",
    "                        labelseq[asp['opinion_word_start']+len(asp['opinion_word'])-1]='E-OW'\n",
    "                        if len(asp['opinion_word'])==1:\n",
    "                            labelseq[asp['opinion_word_start']]='S-OW'\n",
    "            except:\n",
    "                print(data['sentence_id'], 'ow', '{}+{}>{}'.format(asp['opinion_word_start'], len(asp['opinion_word']), len(labelseq)))\n",
    "    if delBIOES:\n",
    "        for i in range(len(labelseq)):\n",
    "            if len(labelseq[i])==3:\n",
    "                labelseq[i]=labelseq[i][2:]\n",
    "    if acsent:\n",
    "        return labelseq, sentiment, category\n",
    "    else:                    \n",
    "        return labelseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination = [(1,0,0,0), (0,1,0,0),(0,0,1,0), (0,0,0,1), (1,1,1,1)]\n",
    "#category = {'O':0, 'B-A':1, 'I-A':2,'B-B':3,'I-B':4, 'B-N':5, 'I-N':6, 'B-O':7,'I-O':8}\n",
    "# category = {'X':0, 'B-A':1, 'I-A':2,'B-B':3,'I-B':4, 'B-N':5, 'I-N':6, 'B-O':7,'I-O':8, 'E-A':9, 'S-A':10, 'E-B':11,'S-B':12,'E-N':13,'S-N':14,'E-O':15,'S-O':16}\n",
    "peiyun_result, muyun_result, yixuan_result, yice_result = [],[],[],[]\n",
    "peiyun_result_sentence, muyun_result_sentence, yixuan_result_sentence, yice_result_sentence = [],[],[],[]\n",
    "peiyun_data, yixuan_data, muyun_data, yice_data = [],[],[],[]\n",
    "contain_icon_data = [93, 931, 1461]\n",
    "\n",
    "for i in labeldata:\n",
    "    if i not in contain_icon_data:\n",
    "        if i <1000:\n",
    "            peiyun_data.append(data1[i])\n",
    "            muyun_data.append(data2[i])\n",
    "        else:\n",
    "            yixuan_data.append(data1[i])\n",
    "            yice_data.append(data2[i])\n",
    "\n",
    "for idx in range(5):\n",
    "    kappa_matrix = []\n",
    "    peiyun_sentence, muyun_sentence, yixuan_sentence, yice_sentence = [], [], [], []\n",
    "    peiyun, muyun, yixuan, yice = [], [], [], []\n",
    "    for i in range(len(peiyun_data)):\n",
    "        peiyun_temp = transfer_to_seq(peiyun_data[i], combination[idx][0],combination[idx][1],combination[idx][2],combination[idx][3])    \n",
    "        muyun_temp = transfer_to_seq(muyun_data[i], combination[idx][0],combination[idx][1],combination[idx][2],combination[idx][3])    \n",
    "        peiyun_sentence.append(peiyun_temp)\n",
    "        muyun_sentence.append(muyun_temp)\n",
    "        for c in range(len(peiyun_data[i]['sentence'])):\n",
    "            peiyun.append(peiyun_temp[c])\n",
    "            muyun.append(muyun_temp[c])\n",
    "\n",
    "    for i in range(len(yixuan_data)):\n",
    "        yixuan_temp = transfer_to_seq(yixuan_data[i], combination[idx][0],combination[idx][1],combination[idx][2],combination[idx][3])    \n",
    "        yice_temp = transfer_to_seq(yice_data[i], combination[idx][0],combination[idx][1],combination[idx][2],combination[idx][3])    \n",
    "        yixuan_sentence.append(yixuan_temp)\n",
    "        yice_sentence.append(yice_temp)\n",
    "        for c in range(len(yixuan_data[i]['sentence'])):\n",
    "            yixuan.append(yixuan_temp[c])\n",
    "            yice.append(yice_temp[c])\n",
    "            \n",
    "    peiyun_result_sentence.append(peiyun_sentence)\n",
    "    muyun_result_sentence.append(muyun_sentence)\n",
    "    yixuan_result_sentence.append(yixuan_sentence)\n",
    "    yice_result_sentence.append(yice_sentence)    \n",
    "    \n",
    "    peiyun_result.append(peiyun)\n",
    "    muyun_result.append(muyun)\n",
    "    yixuan_result.append(yixuan)\n",
    "    yice_result.append(yice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'peiyun_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3989d9fd1115>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#不考慮BIOES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# result = [peiyun_result, muyun_result, yixuan_result, yice_result]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'姵芸 & 慕耘({})\\t:'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeiyun_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcohen_kappa_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeiyun_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmuyun_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'peiyun_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "#labelmember = ['姵芸', '怡萱', '意慈', '慕耘']\n",
    "#不考慮BIOES\n",
    "# result = [peiyun_result, muyun_result, yixuan_result, yice_result]\n",
    "print('姵芸 & 慕耘({})\\t:'.format(len(peiyun_data)), end=' ')\n",
    "for i in range(5):\n",
    "    print(round(cohen_kappa_score(peiyun_result[i],muyun_result[i]),3), end='\\t')\n",
    "print()    \n",
    "print('怡萱 & 意慈({})\\t:'.format(len(yixuan_data)), end=' ')\n",
    "for i in range(5):\n",
    "    print(round(cohen_kappa_score(yixuan_result[i],yice_result[i]),3), end='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment kappa: 0.7002328132509203\n",
      "category kappa: 0.6491543633867651\n",
      "     負向    中立   正向\n",
      "負向  478    78   11\n",
      "中立   60  1204  282\n",
      "正向   13   143  871\n",
      "     配件   功能  外觀   品質  音量   售後    無   價位\n",
      "配件  172   22   5   20   2    3   26    3\n",
      "功能   33  454  20   53   0    2   67    3\n",
      "外觀    5    9  81    3   0    0    6    1\n",
      "品質   21   86  20  350   4   16   66   10\n",
      "音量    3    1   0   10  73    3    3    2\n",
      "售後    7    3   2    8   2  117   27    3\n",
      "無    19   67  16  106   1   24  928   12\n",
      "價位    3    8   1    4   0    2   13  109\n",
      "sentiment same ratio: 0.8130573248407643\n",
      "AC same ratio: 0.7273885350318471\n"
     ]
    }
   ],
   "source": [
    "#count kappa:\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "pb1sentiment, pb2sentiment, pb1category, pb2category=[], [], [], []\n",
    "diff = []\n",
    "for i in range(2000):\n",
    "    pb1, pb2 = [], []\n",
    "    for asp in datafilt1[i]['aspect']:\n",
    "        if asp['product_brand_start']!=-1:\n",
    "            pb1.append(str(asp['product_brand_start'])+asp['product_brand'])\n",
    "    for asp in datafilt2[i]['aspect']:\n",
    "        if asp['product_brand_start']!=-1:\n",
    "            pb2.append(str(asp['product_brand_start'])+asp['product_brand'])\n",
    "    pb1 = set(pb1)\n",
    "    pb2 = set(pb2)\n",
    "    pb1senti, pb2senti = {}, {}\n",
    "    pb1cate, pb2cate = {}, {}\n",
    "    pb = pb1.intersection(pb2)\n",
    "    for asp in datafilt1[i]['aspect']:\n",
    "        if str(asp['product_brand_start'])+asp['product_brand'] in pb:\n",
    "            pb1senti[str(asp['product_brand_start'])+asp['product_brand']]=asp['sentiment']\n",
    "            pb1cate[str(asp['product_brand_start'])+asp['product_brand']]=asp['aspect_category']\n",
    "    for asp in datafilt2[i]['aspect']:\n",
    "        if str(asp['product_brand_start'])+asp['product_brand'] in pb:\n",
    "            pb2senti[str(asp['product_brand_start'])+asp['product_brand']]=asp['sentiment']\n",
    "            pb2cate[str(asp['product_brand_start'])+asp['product_brand']]=asp['aspect_category']\n",
    "    for p in pb:\n",
    "        if pb1senti[p]!=pb2senti[p]:\n",
    "            diff.append(i)\n",
    "        pb1sentiment.append(pb1senti[p])\n",
    "        pb2sentiment.append(pb2senti[p])\n",
    "        pb1category.append(pb1cate[p])\n",
    "        pb2category.append(pb2cate[p])\n",
    "#---\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "print('sentiment kappa:',cohen_kappa_score(pb1sentiment, pb2sentiment))\n",
    "print('category kappa:',cohen_kappa_score(pb1category, pb2category))\n",
    "\n",
    "#confusion matrix of sentiment\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "cm=metrics.confusion_matrix(pb1sentiment,pb2sentiment, labels=list(set(pb1sentiment)))\n",
    "tempname = []\n",
    "for i in pb1sentiment:\n",
    "    if i not in tempname:\n",
    "        tempname.append(i)\n",
    "df_cm = pd.DataFrame(cm, index = list(set(pb1sentiment)),\n",
    "                  columns = list(set(pb1sentiment)))\n",
    "print(df_cm)\n",
    "#---\n",
    "#confusion matrix of sentiment\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "cm=metrics.confusion_matrix(pb1category,pb2category, labels=list(set(pb1category)))\n",
    "tempname = []\n",
    "for i in pb1category:\n",
    "    if i not in tempname:\n",
    "        tempname.append(i)\n",
    "df_cm = pd.DataFrame(cm, index = list(set(pb1category)),\n",
    "                  columns = list(set(pb1category)))\n",
    "print(df_cm)\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "print('sentiment same ratio:', accuracy_score(pb1sentiment, pb2sentiment))\n",
    "print('AC same ratio:', accuracy_score(pb1category, pb2category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect category size: 4925\n",
      "category kappa: 0.6908588079474802\n",
      "category same: 0.7652791878172589\n",
      "sentiment size: 3601\n",
      "sentiment kappa: 0.7237021370682235\n",
      "sentiment same: 0.8381005276312136\n"
     ]
    }
   ],
   "source": [
    "#Count kappa \n",
    "#AC: for same PB PN\n",
    "data_one, data_two = [], []\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "one_ac, two_ac = [], []\n",
    "for dta_idx in range(2000):\n",
    "    one_dict, two_dict = {}, {}\n",
    "    for asp in data1[dta_idx]['aspect']:\n",
    "        target = asp['product_brand']+asp['product_name']+str(asp['product_name_start'])+str(asp['product_brand_start'])\n",
    "        if target not in one_dict.keys():\n",
    "            one_dict[target]=[]\n",
    "        one_dict[target].append(asp['aspect_category'])\n",
    "        \n",
    "    for asp in data2[dta_idx]['aspect']:\n",
    "        target = asp['product_brand']+asp['product_name']+str(asp['product_name_start'])+str(asp['product_brand_start'])\n",
    "        if target not in two_dict.keys():\n",
    "            two_dict[target]=[]\n",
    "        two_dict[target].append(asp['aspect_category'])\n",
    "    for key in one_dict.keys():\n",
    "        one_dict[key] = set(one_dict[key])\n",
    "    for key in two_dict.keys():\n",
    "        two_dict[key] = set(two_dict[key])\n",
    "    for key in set(one_dict.keys()).intersection(two_dict.keys()):\n",
    "        one_temp_dict = {k:False for k in cate.keys()}\n",
    "        two_temp_dict = {k:False for k in cate.keys()}\n",
    "        for k in one_dict[key]:\n",
    "            one_temp_dict[k]=True\n",
    "        for k in two_dict[key]:\n",
    "            two_temp_dict[k]=True\n",
    "        for k in cate.keys(): #先把兩個標記一樣的處理掉\n",
    "            if one_temp_dict[k] and two_temp_dict[k]:\n",
    "                one_ac.append(k)\n",
    "                two_ac.append(k)\n",
    "                one_temp_dict[k]=False\n",
    "                two_temp_dict[k]=False\n",
    "        for k in cate.keys():\n",
    "            if one_temp_dict[k]:\n",
    "                flag = True\n",
    "                for tk in cate.keys():\n",
    "                    if two_temp_dict[tk]:\n",
    "                        one_ac.append(k)\n",
    "                        two_ac.append(tk)\n",
    "                        one_temp_dict[k]=False\n",
    "                        two_temp_dict[tk]=False\n",
    "                        flag=False\n",
    "                        break\n",
    "                if flag:\n",
    "                    one_ac.append(k)\n",
    "                    two_ac.append('無')\n",
    "                    one_temp_dict[k]=False                    \n",
    "            elif two_temp_dict[k]:\n",
    "                flag = True\n",
    "                for tk in cate.keys():\n",
    "                    if one_temp_dict[tk]:\n",
    "                        one_ac.append(tk)\n",
    "                        two_ac.append(k)\n",
    "                        one_temp_dict[tk]=False\n",
    "                        two_temp_dict[k]=False\n",
    "                        flag=False\n",
    "                        break\n",
    "                if flag:\n",
    "                    one_ac.append('無')\n",
    "                    two_ac.append(k)\n",
    "                    two_temp_dict[k]=False                    \n",
    "        for k in cate.keys():\n",
    "            if one_temp_dict[k] or two_temp_dict[k]:\n",
    "                print('error!')\n",
    "##         原本的方法對於兩個標記不相同的會分開來看 e.g. label1 = [功能], label2 = [品質] 會變成 label1[功能 無] label2[無 品質]                \n",
    "#         for ac in one_dict[key].union(two_dict[key]):\n",
    "#             if ac in list(one_dict[key]) and ac in list(two_dict[key]):\n",
    "#                 one_ac.append(ac)\n",
    "#                 two_ac.append(ac)\n",
    "#             elif ac in list(one_dict[key]) and ac not in list(two_dict[key]):\n",
    "#                 one_ac.append(ac)\n",
    "#                 two_ac.append('無')\n",
    "#             elif ac not in list(one_dict[key]) and ac in list(two_dict[key]):\n",
    "#                 one_ac.append('無')\n",
    "#                 two_ac.append(ac)\n",
    "print('aspect category size:', len(one_ac))\n",
    "print('category kappa:',cohen_kappa_score(one_ac, two_ac))\n",
    "print('category same:', accuracy_score(one_ac, two_ac))\n",
    "\n",
    "one_sent, two_sent = [], []\n",
    "err1, err2 = [], []\n",
    "cate = {'音量':0, '價位':1, '外觀':2, '售後':3, '配件':4, '品質':5, '功能':6, '無':7}\n",
    "senti = {'負向':0, '正向':1, '中立':2}\n",
    "for dta_idx in range(2000):\n",
    "    one_dict, two_dict = {}, {}\n",
    "    for asp in data1[dta_idx]['aspect']:\n",
    "        target = asp['product_brand']+asp['product_name']+asp['aspect_category']+str(asp['product_name_start'])+str(asp['product_brand_start'])\n",
    "        if target not in one_dict.keys():\n",
    "            one_dict[target]=asp['sentiment']\n",
    "        else:\n",
    "            if senti[one_dict[target]]>senti[asp['sentiment']]:\n",
    "                one_dict[target]=asp['sentiment']\n",
    "                \n",
    "            if one_dict[target]!=asp['sentiment'] and '中'not in one_dict[target] and '中'not in asp['sentiment']:\n",
    "                err1.append(dta_idx)\n",
    "        \n",
    "    for asp in data2[dta_idx]['aspect']:\n",
    "        target = asp['product_brand']+asp['product_name']+asp['aspect_category']+str(asp['product_name_start'])+str(asp['product_brand_start'])\n",
    "        if target not in two_dict.keys():\n",
    "            two_dict[target]=asp['sentiment']\n",
    "        else:\n",
    "            if senti[two_dict[target]]>senti[asp['sentiment']]:\n",
    "                two_dict[target]=asp['sentiment']\n",
    "                \n",
    "            if two_dict[target]!=asp['sentiment'] and '中'not in two_dict[target] and '中'not in asp['sentiment']:\n",
    "                err2.append(dta_idx)\n",
    "    for key in set(one_dict.keys()).intersection(two_dict.keys()):\n",
    "        one_sent.append(one_dict[key])\n",
    "        two_sent.append(two_dict[key])\n",
    "print('sentiment size:', len(one_sent))\n",
    "print('sentiment kappa:',cohen_kappa_score(one_sent, two_sent))\n",
    "print('sentiment same:', accuracy_score(one_sent, two_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append 4140 data\n",
      "append 3390 data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def swap(a, b):\n",
    "    return b, a\n",
    "def cut_string(sentence, str1, start1, str2='', start2=-1, bound=0):\n",
    "    if start1==-1:\n",
    "        start1=start2\n",
    "    elif start2==-1:\n",
    "        start2=start1\n",
    "        str2=str1\n",
    "    elif start1>start2:\n",
    "        start1, start2 = swap(start1, start2)\n",
    "        str1, str2 = swap(str1, str2)\n",
    "    start=start1\n",
    "    end = start2+len(str2)\n",
    "    \n",
    "    if bound>0:\n",
    "        start = max(0, start-bound)\n",
    "        end = min(len(sentence)-1, end+bound)\n",
    "\n",
    "    return sentence[start:end]\n",
    "train_rel, test_rel = [], []\n",
    "test_rel_intersection = []\n",
    "\n",
    "test_rel_cate, train_rel_cate = [], []\n",
    "train_sentences, test_sentences = [], []\n",
    "\n",
    "train_rel_intersection = []\n",
    "rel_sentences = []\n",
    "error_target = []\n",
    "test_sample = list(np.load('LC_test1000samples_idx.npy'))\n",
    "train_sample = [i for i in range(2000) if i not in test_sample]\n",
    "data1ids = []\n",
    "count = 0\n",
    "bounds = 0\n",
    "pbac_dict = []\n",
    "for dta in data1:\n",
    "    tmppb = []\n",
    "    id_ = []\n",
    "    tmp_dict = {}\n",
    "    for asp in dta['aspect']:\n",
    "        sentence = dta['sentence']\n",
    "        target = ''\n",
    "        if asp['product_brand_start']!=-1:\n",
    "            target = cut_string(sentence, asp['product_brand'], asp['product_brand_start'], bound=bounds)\n",
    "#             target = cut_string(sentence, asp['product_brand'], asp['product_brand_start'], asp['product_name'], asp['product_name_start'], bound=bounds)\n",
    "            if target!='':   \n",
    "                input_data = {'sentence':sentence, 'target':target, 'aspect_category':asp['aspect_category'], 'sentiment':asp['sentiment']}\n",
    "                temp_id = target+asp['aspect_category']+asp['sentiment']                \n",
    "#                 temp_id = target+asp['aspect_category']\n",
    "                if temp_id not in id_:\n",
    "                    count+=1\n",
    "                    if int(dta['sentence_id']) in train_sample:\n",
    "                        train_rel.append(input_data)\n",
    "                    else:\n",
    "                        test_rel.append(input_data)\n",
    "\n",
    "                    rel_sentences.append(input_data)\n",
    "                    id_.append(temp_id)\n",
    "            else:                \n",
    "                error_target.append({'sentence_id':dta['sentence_id'], 'sentence':sentence, 'aspect':asp})\n",
    "    data1ids.append(id_)\n",
    "print('append {} data'.format(count))\n",
    "count = 0\n",
    "for dta in data2:\n",
    "    tmp_dict = {}\n",
    "    for asp in dta['aspect']:\n",
    "        sentence = dta['sentence']\n",
    "        target = ''\n",
    "        if asp['product_brand_start']!=-1:\n",
    "            target = cut_string(sentence, asp['product_brand'], asp['product_brand_start'], bound=bounds)\n",
    "#             target = cut_string(sentence, asp['product_brand'], asp['product_brand_start'], asp['product_name'], asp['product_name_start'], bound=bounds)\n",
    "            if target!='':   \n",
    "                input_data = {'sentence':sentence, 'target':target, 'aspect_category':asp['aspect_category'], 'sentiment':asp['sentiment']}\n",
    "                _id = target+asp['aspect_category']+asp['sentiment']\n",
    "#                 _id = target+asp['aspect_category']\n",
    "                if _id not in data1ids[int(dta['sentence_id'])]:\n",
    "                    count+=1\n",
    "                    if int(dta['sentence_id']) in train_sample:\n",
    "                        train_rel.append(input_data)\n",
    "                    else:\n",
    "                        test_rel.append(input_data)\n",
    "                    \n",
    "                    rel_sentences.append(input_data)\n",
    "                else: #交集\n",
    "                    if asp['product_brand'] not in tmp_dict.keys():\n",
    "                        tmp_dict[asp['product_brand']] = []\n",
    "                    tmp_dict[asp['product_brand']].append(asp['aspect_category'])\n",
    "                    \n",
    "                    if int(dta['sentence_id']) in train_sample :                        \n",
    "                        train_rel_intersection.append(input_data)\n",
    "                        data1ids[int(dta['sentence_id'])].remove(_id)\n",
    "                    else:\n",
    "                        test_rel_intersection.append(input_data)\n",
    "                        data1ids[int(dta['sentence_id'])].remove(_id)\n",
    "            else:\n",
    "                error_target.append({'sentence_id':dta['sentence_id'], 'sentence':sentence, 'aspect':asp})\n",
    "    if tmp_dict!={}:\n",
    "        if int(dta['sentence_id']) in train_sample :                        \n",
    "            train_rel_cate.append(tmp_dict)\n",
    "            train_sentences.append(dta['sentence'])\n",
    "        else:\n",
    "            test_rel_cate.append(tmp_dict)\n",
    "            test_sentences.append(dta['sentence'])\n",
    "print('append {} data'.format(count))\n",
    "\n",
    "#modify for test_rel data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT-pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:1345; test:1422\n",
      "QA-M Train 共寫了1345個份資料\n",
      "QA-M Test 共寫了1422個份資料\n"
     ]
    }
   ],
   "source": [
    "#for sentiment:\n",
    "#BN 產品的 AC 面向是 Sentiment 情感\n",
    "sentiments = ['正向', '負向', '中立']\n",
    "train_aux_sent, test_aux_sent = [], []\n",
    "target = 'aspect_category'\n",
    "type_ = ['NLI-M', 'NLI-B', 'QA-M', 'QA-B']\n",
    "type_ = 'QA-M'\n",
    "for instance in test_rel_intersection:\n",
    "    if type_=='QA-M':\n",
    "        if instance['aspect_category']=='無':\n",
    "            aux = instance['target']+'是什麼情感?'\n",
    "        else:\n",
    "            aux = instance['target']+'的'+instance['aspect_category']+'面向是什麼情感?'\n",
    "        test_aux_sent.append({'sentence':instance['sentence'], 'target':aux, 'sentiment':instance['sentiment']})\n",
    "    elif type_=='NLI-M':\n",
    "        aux = instance['target']+'-'+instance['aspect_category']\n",
    "        test_aux_sent.append({'sentence':instance['sentence'], 'target':aux, 'sentiment':instance['sentiment']})\n",
    "    elif type_=='NLI-B':\n",
    "        for sent in sentiments:\n",
    "            aux = instance['target']+'-'+instance['aspect_category']+'-'+sent\n",
    "            if sent==instance['sentiment']:\n",
    "                test_aux_sent.append({'sentence':instance['sentence'], 'target':aux, 'sentiment':1})\n",
    "            else:\n",
    "                test_aux_sent.append({'sentence':instance['sentence'], 'target':aux, 'sentiment':0})\n",
    "    elif type_=='QA-B':\n",
    "        if instance['aspect_category']=='無':\n",
    "            aux = instance['target']+'有'\n",
    "        else:\n",
    "            aux = instance['target']+'的'+instance['aspect_category']+'面向有'\n",
    "        for sent in sentiments:\n",
    "            if sent==instance['sentiment']:\n",
    "                test_aux_sent.append({'sentence':instance['sentence'], 'target':aux+sent+'情感', 'sentiment':1})\n",
    "            else:\n",
    "                test_aux_sent.append({'sentence':instance['sentence'], 'target':aux+sent+'情感', 'sentiment':0})\n",
    "                \n",
    "for instance in train_rel_intersection:\n",
    "    if type_=='QA-M':\n",
    "        if instance['aspect_category']=='無':\n",
    "            aux = instance['target']+'是什麼情感?'\n",
    "        else:\n",
    "            aux = instance['target']+'的'+instance['aspect_category']+'面向是什麼情感?'\n",
    "        train_aux_sent.append({'sentence':instance['sentence'], 'target':aux, 'sentiment':instance['sentiment']})\n",
    "    elif type_=='NLI-M':\n",
    "        aux = instance['target']+'-'+instance['aspect_category']\n",
    "        train_aux_sent.append({'sentence':instance['sentence'], 'target':aux, 'sentiment':instance['sentiment']})\n",
    "    elif type_=='NLI-B':\n",
    "        for sent in sentiments:\n",
    "            aux = instance['target']+'-'+instance['aspect_category']+'-'+sent\n",
    "            if sent==instance['sentiment']:\n",
    "                train_aux_sent.append({'sentence':instance['sentence'], 'target':aux, 'sentiment':1})\n",
    "            else:\n",
    "                train_aux_sent.append({'sentence':instance['sentence'], 'target':aux, 'sentiment':0})\n",
    "    elif type_=='QA-B':        \n",
    "        if instance['aspect_category']=='無':\n",
    "            aux = instance['target']+'有'\n",
    "        else:\n",
    "            aux = instance['target']+'的'+instance['aspect_category']+'面向有'\n",
    "        for sent in sentiments:\n",
    "            if sent==instance['sentiment']:\n",
    "                train_aux_sent.append({'sentence':instance['sentence'], 'target':aux+sent+'情感', 'sentiment':1})\n",
    "            else:\n",
    "                train_aux_sent.append({'sentence':instance['sentence'], 'target':aux+sent+'情感', 'sentiment':0})\n",
    "    \n",
    "train_aux_sent[:3], test_aux_sent[:3]\n",
    "print('train:{}; test:{}'.format(len(train_aux_sent), len(test_aux_sent)))\n",
    "##write data\n",
    "from datetime import date\n",
    "today = str(date.today())[-5:]\n",
    "with open('PB-Sent_BERT_same_aux_'+type_+'_train'+today+'.json', 'w', encoding='utf8') as file:\n",
    "    count=0\n",
    "    for d in train_aux_sent:\n",
    "        ret = json.dumps(d, ensure_ascii=False)\n",
    "        file.write(ret)\n",
    "        file.write('\\n')\n",
    "        count+=1\n",
    "    print(type_, 'Train 共寫了{}個份資料'.format(count))\n",
    "with open('PB-Sent_BERT_same_aux_'+type_+'_test'+today+'.json', 'w', encoding='utf8') as file:\n",
    "    count=0\n",
    "    for d in test_aux_sent:\n",
    "        ret = json.dumps(d, ensure_ascii=False)\n",
    "        file.write(ret)\n",
    "        file.write('\\n')\n",
    "        count+=1\n",
    "    print(type_, 'Test 共寫了{}個份資料'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:1001; test:1075\n",
      "NLI-M Train 共寫了1001個份資料\n",
      "NLI-M Test 共寫了1075個份資料\n"
     ]
    }
   ],
   "source": [
    "#for aspect category:\n",
    "#BN 產品的 AC 面向\n",
    "train_sentences, test_sentences\n",
    "train_rel_cate, test_rel_cate\n",
    "categories = ['功能', '品質','配件','售後','外觀','價位','音量', '無']\n",
    "train_aux_cate, test_aux_cate = [], []\n",
    "type_ = ['NLI-M', 'NLI-B', 'QA-M', 'QA-B']\n",
    "type_ = 'NLI-M'\n",
    "cnt = 0\n",
    "for instance in test_rel_cate:\n",
    "    for key, values in instance.items():\n",
    "        if type_=='QA-M':\n",
    "            aux = '敘述'+key+'產品的什麼面向?'\n",
    "            test_aux_cate.append({'sentence':test_sentences[cnt], 'target':aux, 'aspect_category':values})\n",
    "        elif type_=='NLI-M':\n",
    "            aux = key\n",
    "            test_aux_cate.append({'sentence':test_sentences[cnt], 'target':aux, 'aspect_category':values})\n",
    "        elif type_=='NLI-B':\n",
    "            for cate in categories:\n",
    "                aux = key+'-'+cate\n",
    "                if cate in values:\n",
    "                    test_aux_cate.append({'sentence':test_sentences[cnt], 'target':aux, 'aspect_category':1})\n",
    "                else:\n",
    "                    test_aux_cate.append({'sentence':test_sentences[cnt], 'target':aux, 'aspect_category':0})\n",
    "        elif type_=='QA-B':\n",
    "            for cate in categories:\n",
    "                aux = '敘述'+key+'產品的'+cate\n",
    "                if cate in values:\n",
    "                    test_aux_cate.append({'sentence':test_sentences[cnt], 'target':aux, 'aspect_category':1})\n",
    "                else:\n",
    "                    test_aux_cate.append({'sentence':test_sentences[cnt], 'target':aux, 'aspect_category':0})\n",
    "    cnt+=1\n",
    "                \n",
    "cnt = 0\n",
    "for instance in train_rel_cate:\n",
    "    for key, values in instance.items():\n",
    "        if type_=='QA-M':\n",
    "            aux = '敘述'+key+'產品的什麼面向?'\n",
    "            train_aux_cate.append({'sentence':train_sentences[cnt], 'target':aux, 'aspect_category':values})\n",
    "        elif type_=='NLI-M':\n",
    "            aux = key\n",
    "            train_aux_cate.append({'sentence':train_sentences[cnt], 'target':aux, 'aspect_category':values})\n",
    "        elif type_=='NLI-B':\n",
    "            for cate in categories:\n",
    "                aux = key+'-'+cate\n",
    "                if cate in values:\n",
    "                    train_aux_cate.append({'sentence':train_sentences[cnt], 'target':aux, 'aspect_category':1})\n",
    "                else:\n",
    "                    train_aux_cate.append({'sentence':train_sentences[cnt], 'target':aux, 'aspect_category':0})\n",
    "        elif type_=='QA-B':\n",
    "            for cate in categories:\n",
    "                aux = '敘述'+key+'產品的'+cate\n",
    "                if cate in values:\n",
    "                    train_aux_cate.append({'sentence':train_sentences[cnt], 'target':aux, 'aspect_category':1})\n",
    "                else:\n",
    "                    train_aux_cate.append({'sentence':train_sentences[cnt], 'target':aux, 'aspect_category':0})  \n",
    "    cnt+=1\n",
    "train_aux_cate[:3], test_aux_cate[:3]\n",
    "print('train:{}; test:{}'.format(len(train_aux_cate), len(test_aux_cate)))\n",
    "\n",
    "from datetime import date\n",
    "today = str(date.today())[-5:]\n",
    "with open('PB-AC_BERT_same_aux_'+type_+'_train'+today+'.json', 'w', encoding='utf8') as file:\n",
    "    count=0\n",
    "    for d in train_aux_cate:\n",
    "        ret = json.dumps(d, ensure_ascii=False)\n",
    "        file.write(ret)\n",
    "        file.write('\\n')\n",
    "        count+=1\n",
    "    print(type_, 'Train 共寫了{}個份資料'.format(count))\n",
    "with open('PB-AC_BERT_same_aux_'+type_+'_test'+today+'.json', 'w', encoding='utf8') as file:\n",
    "    count=0\n",
    "    for d in test_aux_cate:\n",
    "        ret = json.dumps(d, ensure_ascii=False)\n",
    "        file.write(ret)\n",
    "        file.write('\\n')\n",
    "        count+=1\n",
    "    print(type_, 'Test 共寫了{}個份資料'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into 8 different data by AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tf/notebooks//NER_code/NER_data/homeapp/diff_cate/音量/',\n",
       " '/tf/notebooks//NER_code/NER_data/homeapp/diff_cate/價位/',\n",
       " '/tf/notebooks//NER_code/NER_data/homeapp/diff_cate/外觀/',\n",
       " '/tf/notebooks//NER_code/NER_data/homeapp/diff_cate/售後/',\n",
       " '/tf/notebooks//NER_code/NER_data/homeapp/diff_cate/配件/',\n",
       " '/tf/notebooks//NER_code/NER_data/homeapp/diff_cate/品質/',\n",
       " '/tf/notebooks//NER_code/NER_data/homeapp/diff_cate/功能/',\n",
       " '/tf/notebooks//NER_code/NER_data/homeapp/diff_cate/無/']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate = {'音量':0, '價位':1, '外觀':2, '售後':3, '配件':4, '品質':5, '功能':6, '無':7}\n",
    "cate = list(cate.keys())\n",
    "dir_path = []\n",
    "for c in cate:\n",
    "    dir_path.append('/tf/notebooks//NER_code/NER_data/homeapp/diff_cate/'+c+'/')\n",
    "dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "音量 30\n",
      "價位 49\n",
      "外觀 66\n",
      "售後 73\n",
      "配件 106\n",
      "品質 235\n",
      "功能 375\n",
      "無 411\n",
      "test:\n",
      "音量 37\n",
      "價位 63\n",
      "外觀 69\n",
      "售後 50\n",
      "配件 83\n",
      "品質 204\n",
      "功能 422\n",
      "無 494\n"
     ]
    }
   ],
   "source": [
    "#IN DIFFERENT ASPECT\n",
    "\n",
    "cate = {'音量':0, '價位':1, '外觀':2, '售後':3, '配件':4, '品質':5, '功能':6, '無':7}\n",
    "cate = list(cate.keys())\n",
    "all_count = []\n",
    "for i in range(8):\n",
    "    all_count.append(0)\n",
    "with open(dir_path[0]+\"train.json\",\"w\",encoding=\"utf-8\") as vol, \\\n",
    "    open(dir_path[1]+\"train.json\", \"w\", encoding=\"utf-8\") as price, \\\n",
    "    open(dir_path[2]+\"train.json\", \"w\", encoding=\"utf-8\") as look, \\\n",
    "    open(dir_path[3]+\"train.json\", \"w\", encoding=\"utf-8\") as after, \\\n",
    "    open(dir_path[4]+\"train.json\", \"w\", encoding=\"utf-8\") as pei, \\\n",
    "    open(dir_path[5]+\"train.json\", \"w\", encoding=\"utf-8\") as qua, \\\n",
    "    open(dir_path[6]+\"train.json\", \"w\", encoding=\"utf-8\") as func, \\\n",
    "    open(dir_path[7]+\"train.json\", \"w\",encoding=\"utf-8\") as none:\n",
    "    for d in train_rel_intersection:\n",
    "        if d['aspect_category']==cate[0]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            vol.write(ret)\n",
    "            vol.write('\\n')\n",
    "            all_count[0]+=1\n",
    "        elif d['aspect_category']==cate[1]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            price.write(ret)\n",
    "            price.write('\\n')\n",
    "            all_count[1]+=1\n",
    "        elif d['aspect_category']==cate[2]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            look.write(ret)\n",
    "            look.write('\\n')\n",
    "            all_count[2]+=1\n",
    "        elif d['aspect_category']==cate[3]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            after.write(ret)\n",
    "            after.write('\\n')\n",
    "            all_count[3]+=1\n",
    "        elif d['aspect_category']==cate[4]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            pei.write(ret)\n",
    "            pei.write('\\n')\n",
    "            all_count[4]+=1\n",
    "        elif d['aspect_category']==cate[5]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            qua.write(ret)\n",
    "            qua.write('\\n')\n",
    "            all_count[5]+=1\n",
    "        elif d['aspect_category']==cate[6]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            func.write(ret)\n",
    "            func.write('\\n')\n",
    "            all_count[6]+=1\n",
    "        elif d['aspect_category']==cate[7]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            none.write(ret)\n",
    "            none.write('\\n')\n",
    "            all_count[7]+=1\n",
    "print('train:')\n",
    "for i in range(8):\n",
    "    print(cate[i], all_count[i])\n",
    "\n",
    "all_count = []\n",
    "for i in range(8):\n",
    "    all_count.append(0)\n",
    "with open(dir_path[0]+\"test.json\",\"w\",encoding=\"utf-8\") as vol, \\\n",
    "    open(dir_path[1]+\"test.json\", \"w\", encoding=\"utf-8\") as price, \\\n",
    "    open(dir_path[2]+\"test.json\", \"w\", encoding=\"utf-8\") as look, \\\n",
    "    open(dir_path[3]+\"test.json\", \"w\", encoding=\"utf-8\") as after, \\\n",
    "    open(dir_path[4]+\"test.json\", \"w\", encoding=\"utf-8\") as pei, \\\n",
    "    open(dir_path[5]+\"test.json\", \"w\", encoding=\"utf-8\") as qua, \\\n",
    "    open(dir_path[6]+\"test.json\", \"w\", encoding=\"utf-8\") as func, \\\n",
    "    open(dir_path[7]+\"test.json\", \"w\",encoding=\"utf-8\") as none:\n",
    "    for d in test_rel_intersection:\n",
    "        if d['aspect_category']==cate[0]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            vol.write(ret)\n",
    "            vol.write('\\n')\n",
    "            all_count[0]+=1\n",
    "        elif d['aspect_category']==cate[1]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            price.write(ret)\n",
    "            price.write('\\n')\n",
    "            all_count[1]+=1\n",
    "        elif d['aspect_category']==cate[2]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            look.write(ret)\n",
    "            look.write('\\n')\n",
    "            all_count[2]+=1\n",
    "        elif d['aspect_category']==cate[3]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            after.write(ret)\n",
    "            after.write('\\n')\n",
    "            all_count[3]+=1\n",
    "        elif d['aspect_category']==cate[4]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            pei.write(ret)\n",
    "            pei.write('\\n')\n",
    "            all_count[4]+=1\n",
    "        elif d['aspect_category']==cate[5]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            qua.write(ret)\n",
    "            qua.write('\\n')\n",
    "            all_count[5]+=1\n",
    "        elif d['aspect_category']==cate[6]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            func.write(ret)\n",
    "            func.write('\\n')\n",
    "            all_count[6]+=1\n",
    "        elif d['aspect_category']==cate[7]:\n",
    "            ret = json.dumps(d, ensure_ascii=False)\n",
    "            none.write(ret)\n",
    "            none.write('\\n')\n",
    "            all_count[7]+=1\n",
    "print('test:')\n",
    "for i in range(8):\n",
    "    print(cate[i], all_count[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 創造PN, PB, AT, OW的NE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create labeled data\n",
    "#for NER\n",
    "nerlabel = []\n",
    "for dta in data:\n",
    "    try:\n",
    "        nerlabel.append([list(dta['sentence']), transfer_to_seq(dta, delBIOES=False)])\n",
    "    except:\n",
    "        ;\n",
    "\n",
    "        \n",
    "#write NER data into txt\n",
    "with open('家電NER.txt', encoding='utf8', mode='w') as file:\n",
    "    for i in range(len(nerlabel)):\n",
    "        for a, b in zip(nerlabel[i][0], nerlabel[i][1]):\n",
    "            wr = str(a+'\\t'+b+'\\n')\n",
    "            file.write(wr)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'清掃\\n': 648, '空調\\n': 583, '洗滌\\n': 357, '廚房\\n': 351, '生活\\n': 61})\n"
     ]
    }
   ],
   "source": [
    "#for NER only PB, PN, 分不同的版來存\n",
    "with open('各文章版.txt', encoding='utf8') as file:\n",
    "    paragraph_title = file.readlines()\n",
    "from collections import Counter\n",
    "print(Counter(paragraph_title))\n",
    "idx2title = {i:paragraph_title[i][:-1] for i in range(len(paragraph_title))}\n",
    "nerlabel = {'廚房':[], '清掃':[], '洗滌':[], '空調':[], '生活':[]}\n",
    "count = 0\n",
    "\n",
    "for dta in data:\n",
    "    try:        \n",
    "        nerlabel[idx2title[count]].append([list(dta['sentence']), transfer_to_seq(dta, delBIOES=False)])\n",
    "    except:\n",
    "        ;\n",
    "    count+=1\n",
    "del count\n",
    "\n",
    "label_list = ['廚房', '清掃', '洗滌', '空調', '生活']\n",
    "for label in label_list:\n",
    "    with open(label+'NER.txt', 'w', encoding='utf8') as file:\n",
    "        for i in range(len(nerlabel[label])):\n",
    "            for a, b in zip(nerlabel[label][i][0], nerlabel[label][i][1]):\n",
    "                wr = str(a+'\\t'+b+'\\n')\n",
    "                file.write(wr)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開利 4\n",
      "石頭 2\n",
      "tiddi 1\n",
      "飛利浦 9\n",
      "哈根諾克 3\n",
      "德國雙人 2\n",
      "元山家電 1\n",
      "sakura櫻花牌 1\n",
      "日本正負零 1\n",
      "osim 8\n",
      "禾聯碩 3\n",
      "honeywell 3\n",
      "牛頭牌 1\n",
      "喜特麗 5\n",
      "sanlux 7\n",
      "finlux 1\n",
      "beko 2\n",
      "proscenic浦桑尼克 1\n",
      "vita－mix 1\n",
      "panasonic 國際牌 2\n",
      "喬山 1\n",
      "fuji 4\n",
      "輝葉 2\n",
      "teco 東元 3\n",
      "lg樂金 1\n",
      "sampo 3\n",
      "聯碩 2\n",
      "tefal 法國特福 1\n",
      "惠爾浦 2\n",
      "blomberg 2\n",
      "iris 4\n",
      "象印 1\n",
      "panasonic 台松 1\n",
      "台松 1\n",
      "阿拉斯加 2\n",
      "美的 6\n",
      "中華電信 1\n",
      "google 1\n",
      "富士 4\n",
      "東芝toshiba 1\n",
      "晶工牌 2\n",
      "品諾 2\n",
      "paloma 1\n",
      "amica 3\n",
      "irobot 4\n",
      "莊頭北工業 1\n",
      "hcg 和成 1\n",
      "格力 7\n",
      "森寶 1\n",
      "電光牌 2\n",
      "櫻花牌 5\n",
      "大宇 2\n",
      "松木家電 1\n",
      "cuisinart 1\n",
      "元山 1\n",
      "kitchenaid 1\n",
      "中央牌 8\n",
      "小紫 1\n",
      "electrolux 4\n",
      "大金daikin 1\n",
      "大白 1\n",
      "和成 3\n",
      "t牌 2\n",
      "新禾 4\n",
      "kolin 歌林 1\n",
      "sampo 聲寶 4\n",
      "frigidaire 2\n",
      "台灣三洋 7\n",
      "whirlpool 3\n",
      "上豪 1\n",
      "sony 2\n",
      "優必洗 4\n",
      "teco 1\n",
      "px 2\n",
      "bosch博世 1\n",
      "hitachi日立 3\n",
      "東龍 2\n",
      "東龍牌 1\n",
      "braun 4\n",
      "kenmore 5\n",
      "maytag 2\n",
      "戴森 7\n",
      "五聯 1\n",
      "怡心牌 6\n",
      "poiema 1\n",
      "百靈 2\n",
      "台灣松下 3\n",
      "hitachi 日立 4\n",
      "德國miele 1\n",
      "瑞士stadler form 1\n",
      "海立 1\n",
      "瑞林 1\n",
      "panasonic國際牌 4\n",
      "賀眾牌 9\n",
      "鍋寶 2\n",
      "hcg 4\n",
      "鴻茂 4\n",
      "萬寶松下 1\n",
      "台灣pana 2\n",
      "上海日立 4\n",
      "博世 1\n",
      "sanlux台灣三洋 1\n",
      "magtag 1\n",
      "chimei奇美 1\n",
      "台灣日立 3\n",
      "中國國際牌 1\n",
      "台灣國際牌 1\n",
      "液晶電視 1\n",
      "日立江森 2\n",
      "鑫司 2\n",
      "heran 禾聯 1\n",
      "歌林kolin 1\n",
      "hawrin華菱 1\n",
      "p牌 1\n",
      "松下電器 1\n",
      "松下電器panasonic 1\n",
      "日本三洋 2\n",
      "三陽 2\n",
      "lg 樂金 2\n",
      "samsung三星 1\n",
      "toshiba 東芝 2\n",
      "米家 4\n",
      "mauviel 1\n",
      "科沃斯 1\n",
      "i－robot 1\n",
      "midea 美的 1\n",
      "洗碗機 1\n",
      "三星電子 1\n",
      "海爾 2\n",
      "台灣三菱電機 1\n",
      "廣州萬寶松下 1\n",
      "家樂福 2\n",
      "stadler form 2\n",
      "林內 5\n",
      "小綠 1\n",
      "亮哥牌 1\n",
      "櫻花 2\n",
      "vornado 2\n",
      "重工 4\n",
      "國際日立 1\n",
      "日立冷氣 1\n",
      "日立家電 3\n",
      "日立原廠 1\n",
      "優必洗huebsch 1\n",
      "蘋果 1\n",
      "日本旭日asahi 1\n",
      "日本東芝 2\n",
      "sampo聲寶 1\n",
      "tokuyo 1\n",
      "東元家電 2\n",
      "三線 1\n",
      "二線 1\n",
      "blendtec 1\n",
      "speed queen 2\n",
      "全家安 2\n",
      "多德仕 1\n",
      "philips飛利浦 2\n",
      "philips 飛利浦 2\n",
      "philips 2\n",
      "滾筒 1\n",
      "京造 1\n",
      "電機 1\n",
      "名象 1\n",
      "a.o.smith 1\n",
      "上洋 1\n",
      "呆森 1\n",
      "臺灣lg 1\n",
      "國x牌 1\n",
      "東芝twd 1\n",
      "樂金 1\n",
      "bh 1\n",
      "日立hitachi 2\n",
      "日本日立 4\n",
      "miele gaggenau 1\n",
      "日本東芝toshiba 1\n",
      "追覓 dreame 1\n",
      "三菱\"重工\" 1\n",
      "追覓 1\n",
      "電視 1\n",
      "燈泡 1\n",
      "canon 1\n",
      "日本國際牌 1\n",
      "jvc 1\n",
      "花牌 1\n",
      "swift 2\n",
      "德國bosch 1\n",
      "三菱家電 3\n",
      "brita 1\n",
      "coway 2\n",
      "braun百靈 1\n",
      "magimix 1\n",
      "vitamix 1\n",
      "富士電通 1\n",
      "teco東元 3\n",
      "白朗 1\n",
      "liebherr 1\n",
      "德國blomberg 1\n",
      "法國湯姆笙 1\n",
      "三菱電壓 1\n",
      "toshiba東芝 2\n",
      "hom－bot 1\n",
      "mature美萃 2\n",
      "日立牌 1\n",
      "iq air 1\n",
      "pinoh 1\n",
      "雲米 1\n",
      "台熱牌 tew 1\n",
      "宜得利 1\n",
      "順光 1\n",
      "九陽 1\n",
      "日本panasonic 2\n",
      "美泰客 1\n",
      "東寶電器 1\n",
      "±0 1\n",
      "台熱牌 1\n",
      "德國海豚 1\n",
      "聲寶sampo 1\n",
      "台灣惠而浦 3\n",
      "利勃 1\n",
      "liebherr 利勃 1\n",
      "twinbird 2\n",
      "小狗 1\n",
      "x內 1\n",
      "x花 1\n",
      "cooper 1\n",
      "d牌 1\n",
      "大河 1\n",
      "美國富及第 1\n",
      "夏普國際 1\n",
      "美萃 2\n",
      "伊克斯 1\n",
      "ih 1\n",
      "gaggenau 1\n",
      "＋－0 1\n",
      "香港 伊萊克斯 1\n",
      "伊娜卡 1\n"
     ]
    }
   ],
   "source": [
    "#產品名稱提及次數\n",
    "lessthread = 10\n",
    "validthread = 40\n",
    "target_ = 'product_brand'\n",
    "products = []\n",
    "for dta in data:\n",
    "    myproduct = []\n",
    "    product = []\n",
    "    for asp in dta['aspect']:\n",
    "        if asp['product_brand_start']!=-1:\n",
    "            myproduct.append(asp[target_]+'+'+str(asp[target_+'_start']))\n",
    "    myproduct = list(set(myproduct))\n",
    "    for p in myproduct:\n",
    "        product.append(p.split('+')[0])\n",
    "    products+=product\n",
    "from collections import Counter\n",
    "#print(Counter(products))\n",
    "for i in Counter(products):\n",
    "    if Counter(products)[i]<lessthread:\n",
    "        print(i, Counter(products)[i])\n",
    "lessproduct = []\n",
    "validproduct = []\n",
    "for p in dict(Counter(products)):\n",
    "#    print(p,'\\t', Counter(products)[p])\n",
    "    if  Counter(products)[p]<lessthread:\n",
    "        lessproduct.append(p)\n",
    "for p in dict(Counter(products)):\n",
    "#    print(p,'\\t', Counter(products)[p])\n",
    "    if  Counter(products)[p]<validthread and Counter(products)[p]>lessthread:\n",
    "        validproduct.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source domain共寫了1335筆資料\n",
      "validation domain共寫了192筆資料\n",
      "target domain共寫了473筆資料\n"
     ]
    }
   ],
   "source": [
    "#create labeled data\n",
    "#for NER\n",
    "src_nerlabel, valid_nerlabel, tgt_nerlabel = [], [], []\n",
    "for dta in data:\n",
    "    if int(dta['sentence_id']) in tgtdomain:\n",
    "        try:\n",
    "            tgt_nerlabel.append([list(dta['sentence']), transfer_to_seq(dta, at=False, ow=False,delBIOES=False)])\n",
    "        except:\n",
    "            ;\n",
    "    elif int(dta['sentence_id']) in valdomain:\n",
    "        try:\n",
    "            valid_nerlabel.append([list(dta['sentence']), transfer_to_seq(dta, at=False, ow=False,delBIOES=False)])\n",
    "        except:\n",
    "            ;\n",
    "    else:\n",
    "        try:\n",
    "            src_nerlabel.append([list(dta['sentence']), transfer_to_seq(dta, at=False, ow=False,delBIOES=False)])\n",
    "        except:\n",
    "            ;\n",
    "        \n",
    "        \n",
    "#write NER data into txt\n",
    "with open('srcNER.txt', encoding='utf8', mode='w') as file:\n",
    "    for i in range(len(src_nerlabel)):\n",
    "        for a, b in zip(src_nerlabel[i][0], src_nerlabel[i][1]):\n",
    "            wr = str(a+'\\t'+b+'\\n')\n",
    "            file.write(wr)\n",
    "        file.write('\\n')\n",
    "    print('source domain共寫了{}筆資料'.format(len(src_nerlabel)))\n",
    "with open('valNER.txt', encoding='utf8', mode='w') as file:\n",
    "    for i in range(len(valid_nerlabel)):\n",
    "        for a, b in zip(valid_nerlabel[i][0], valid_nerlabel[i][1]):\n",
    "            wr = str(a+'\\t'+b+'\\n')\n",
    "            file.write(wr)\n",
    "        file.write('\\n')\n",
    "    print('validation domain共寫了{}筆資料'.format(len(valid_nerlabel)))\n",
    "with open('tgtNER.txt', encoding='utf8', mode='w') as file:\n",
    "    for i in range(len(tgt_nerlabel)):\n",
    "        for a, b in zip(tgt_nerlabel[i][0], tgt_nerlabel[i][1]):\n",
    "            wr = str(a+'\\t'+b+'\\n')\n",
    "            file.write(wr)\n",
    "        file.write('\\n')\n",
    "    print('target domain共寫了{}筆資料'.format(len(tgt_nerlabel)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把存檔重新寫回資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把存檔重新寫回資料庫\n",
    "#在執行之前要先把database裡面的資料刪掉，這邊的程式要再自己加上去哟！\n",
    "dest = '2021-01-06 13h07m37s(old)'##這裡要自己更改\n",
    "import json\n",
    "with open(dest+'\\label1.json', encoding='utf8') as file:\n",
    "    datatemp = file.readlines()\n",
    "data1 = []\n",
    "for dta in datatemp:\n",
    "    data1.append(json.loads(dta))\n",
    "    \n",
    "with open(dest+'\\label2.json', encoding='utf8') as file:\n",
    "    datatemp = file.readlines()\n",
    "data2 = []\n",
    "for dta in datatemp:\n",
    "    data2.append(json.loads(dta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data1:\n",
    "    collection1.insert_one(i)\n",
    "for i in data2:\n",
    "    collection2.insert_one(i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://admin:widmwidm9527@140.115.54.44:27017')\n",
    "collection1 = db['KKBOX_Label_Ken1']\n",
    "cursor1 = collection1.find({}) \n",
    "data1 = [d for d in cursor1] \n",
    "\n",
    "collection2 = db['KKBOX_Label_Ken2']\n",
    "cursor2 = collection2.find({}) \n",
    "data2 = [d for d in cursor2] \n",
    "data1 = sorted(data1, key = lambda x: int(x['sentence_id']))\n",
    "data2 = sorted(data2, key = lambda x: int(x['sentence_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_categories = []\n",
    "categories ={'功能':0, '品質':1,'配件':2,'售後':3,'外觀':4,'價位':5,'音量':6}\n",
    "for dta in data:\n",
    "    temp_cate = [0, 0, 0, 0, 0, 0, 0]\n",
    "    for asp in dta['aspect']:\n",
    "        if asp['aspect_category']!='無':\n",
    "            temp_cate[categories[asp['aspect_category']]]=1\n",
    "    aspect_categories.append(np.array(temp_cate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新data 的資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把combine data 寫到port 7999\n",
    "#先把collection清空\n",
    "collection = db['KKBOX_Label_Ken']\n",
    "collection.delete_many({})\n",
    "for i in data:\n",
    "    collection.insert_one(i)\n",
    "cursor = collection.find({}) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
